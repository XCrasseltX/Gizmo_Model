"""
Gizmo Python LLM Server
Orchestriert: HA ‚Üí C++ Coach ‚Üí Gemini ‚Üí HA
"""
import asyncio
import aiohttp
import logging
from typing import Optional, AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import httpx
import subprocess
import shlex
import json
import os
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
import redis

load_dotenv()

# --- Port-Konfiguration ---
PORT_PYTHON_SERVER = 5000     # FastAPI Server
PORT_CPP_COACH = 5001         # C++ Coach Server
PORT_MCP_GATEWAY = 5002       # MCP Gateway (Docker oder lokal)
PORT_REDIS = 5003             # Redis Database Port

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("gizmo_server")

# Configuration
CPP_COACH_URL = os.getenv("CPP_COACH_URL", f"http://localhost:{PORT_CPP_COACH}")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "your-api-key-here")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
REDIS_URL = os.getenv("REDIS_URL", f"redis://localhost:{PORT_REDIS}/0")

app = FastAPI(title="Gizmo AI Server", version="1.0.1")


# Request Models
class ConversationRequest(BaseModel):
    text: str
    conversation_id: str
    language: str = "de"


class ConversationResponse(BaseModel):
    response: str
    conversation_id: str
    intent: Optional[dict] = None


class HealthResponse(BaseModel):
    status: str
    cpp_coach_available: bool
    gemini_configured: bool

# --- NEU: Redis Client f√ºr Memory ---
class ConversationMemory:
    """Speichert und ruft den Konversationsverlauf (contents) in Redis ab."""
    
    def __init__(self, redis_url: str):
        self.r = redis.from_url(redis_url, decode_responses=True)
        self.ttl_seconds = 3600  # Verlauf nach 1 Stunde l√∂schen (Time To Live)

    def _key(self, conversation_id: str) -> str:
        return f"gizmo:conv:{conversation_id}"

    async def load_history(self, conversation_id: str) -> list:
        try:
            key = self._key(conversation_id)
            data = await asyncio.to_thread(self.r.get, key)
            if data:
                return json.loads(data)
            return []
        except Exception as e:
            logger.warning(f"Redis unavailable, using empty history: {e}")
            return []  # Fallback auf leeren Verlauf

    async def save_history(self, conversation_id: str, contents: list):
        try:
            key = self._key(conversation_id)
            data = json.dumps(contents)
            await asyncio.to_thread(self.r.setex, key, self.ttl_seconds, data)
        except Exception as e:
            logger.warning(f"Could not save to Redis: {e}")
        
    async def connect(self):
        """Einfacher Check, ob Redis verf√ºgbar ist"""
        try:
            await asyncio.to_thread(self.r.ping)
            logger.info("‚úÖ Redis Memory Client verbunden")
            return True
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Redis Memory Client Fehler: {e}")
            return False


# --- C++ Coach Client ---
class CppCoachClient:
    """Kommunikation mit dem C++ Coach ‚Äî holt angereicherten Prompt & Hormonwerte"""

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.timeout = httpx.Timeout(10.0, connect=5.0)

    async def get_enriched_prompt(self):
        """Ruft C++ Coach auf und gibt dict mit 'prompt' und 'emotion' zur√ºck"""
        try:
            logger.info(f"Rufe C++ Coach auf f√ºr")
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                payload = {
                    "method": "get_prompt_context",
                    "id": 1
                }
                response = await client.post(
                    self.base_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                response.raise_for_status()
                data = response.json()

                result = data.get("result", {})
                prompt = result.get("prompt")
                emotion = result.get("emotion", {})

                if not prompt:
                    logger.warning("Coach gab keinen Prompt zur√ºck, nutze Fallback")
                    prompt = f"Du bist Gizmo. Der Nutzer fragt:"

                logger.info(f"Prompt erhalten: {prompt[:100]}...")
                return {"prompt": prompt, "emotion": emotion}

        except httpx.TimeoutException:
            logger.error("Timeout beim C++ Coach")
            raise HTTPException(status_code=504, detail="C++ Coach Timeout")
        except httpx.HTTPError as e:
            logger.error(f"HTTP Fehler beim C++ Coach: {e}")
            raise HTTPException(status_code=502, detail=f"C++ Coach Error: {str(e)}")
        except Exception as e:
            logger.exception("Unerwarteter Fehler beim C++ Coach")
            raise HTTPException(status_code=500, detail=f"Coach Error: {str(e)}")

# --- Docker MCP Client (verbindet sich mit laufendem Gateway) ---
class MCPGatewayClient:
    """Verbindet sich mit dem MCP Gateway √ºber Streamable HTTP Protocol"""
    
    def __init__(self, gateway_host: str = "localhost", gateway_port: int = PORT_MCP_GATEWAY):
        self.gateway_url = f"http://{gateway_host}:{gateway_port}"
        self.mcp_endpoint = "/mcp"  # <-- NEU: Endpoint definieren
        self.client = None
        self.tools = []
        self.message_id = 0
        self.session_id = None
    
    async def connect(self):
        """Verbinde mit dem Gateway"""
        try:
            import httpx
            self.client = httpx.AsyncClient(
                base_url=self.gateway_url,
                timeout=30.0
            )
            
            logger.info(f"‚úÖ Verbunden mit MCP Gateway auf {self.gateway_url}")
            
            # Initialize Session
            await self._initialize_session()
            
            return True
            
        except Exception as e:
            logger.error(f"Gateway Verbindung fehlgeschlagen: {e}")
            return False
    
    async def _initialize_session(self):
        """Initialize MCP Session"""
        try:
            result = await self._send_request("initialize", {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {
                    "name": "gizmo-server",
                    "version": "1.0.0"
                }
            })
            
            if result:
                logger.info("‚úÖ MCP Session initialisiert")
                # Send initialized notification
                await self._send_notification("notifications/initialized")
            
        except Exception as e:
            logger.error(f"Session Init Error: {e}")
    
    async def _send_request(self, method: str, params: dict = None):
        """Sende MCP Request √ºber HTTP POST und warte auf Response"""
        try:
            self.message_id += 1
            
            request_body = {
                "jsonrpc": "2.0",
                "id": self.message_id,
                "method": method
            }
            
            if params:
                request_body["params"] = params
            
            headers = {
                "Accept": "application/json, text/event-stream",
                "Content-Type": "application/json"
            }
            
            if self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            logger.info(f"üîß Sende MCP Request: {method}")
            
            # POST Request an root endpoint
            response = await self.client.post(
                self.mcp_endpoint,  # <-- Statt "/" verwende "/mcp"
                json=request_body,
                headers=headers
            )
            
            logger.info(f"Response status: {response.status_code}")
            logger.info(f"Response headers: {dict(response.headers)}")
            
            # Check for session ID in response
            if "Mcp-Session-Id" in response.headers:
                self.session_id = response.headers["Mcp-Session-Id"]
                logger.info(f"Session ID: {self.session_id}")
            
            # Check Content-Type
            content_type = response.headers.get("content-type", "")
            
            if "text/event-stream" in content_type:
                # Server sendet SSE Stream
                logger.info("Empfange SSE Stream...")
                return await self._read_sse_response(response)
            else:
                # Server sendet JSON Response
                logger.info("Empfange JSON Response...")
                data = response.json()
                
                if "error" in data:
                    logger.error(f"MCP Error: {data['error']}")
                    return None
                
                return data.get("result")
            
        except Exception as e:
            logger.exception(f"Request Error f√ºr {method}")
            return None
    
    async def _read_sse_response(self, response):
        """Lese SSE Stream und extrahiere Response"""
        try:
            result = None
            
            async for line in response.aiter_lines():
                if not line or not line.startswith("data: "):
                    continue
                
                data_str = line[6:].strip()
                if not data_str or data_str == "[DONE]":
                    continue
                
                try:
                    event = json.loads(data_str)
                    
                    # Pr√ºfe ob es unsere Response ist
                    if event.get("id") == self.message_id:
                        if "error" in event:
                            logger.error(f"MCP Error: {event['error']}")
                            return None
                        
                        result = event.get("result")
                        break
                        
                except json.JSONDecodeError:
                    logger.warning(f"Konnte JSON nicht dekodieren: {data_str}")
                    continue
            
            return result
            
        except Exception as e:
            logger.exception("SSE Read Error")
            return None
    
    async def _send_notification(self, method: str, params: dict = None):
        """Sende MCP Notification (keine Response erwartet)"""
        try:
            request_body = {
                "jsonrpc": "2.0",
                "method": method
            }
            
            if params:
                request_body["params"] = params
            
            headers = {
                "Content-Type": "application/json"
            }
            
            if self.session_id:
                headers["Mcp-Session-Id"] = self.session_id
            
            await self.client.post(
                self.mcp_endpoint,  # <-- Auch hier
                json=request_body,
                headers=headers
            )
            
        except Exception as e:
            logger.error(f"Notification Error: {e}")
    
    async def fetch_tools(self):
        """Hole Tools √ºber MCP Protokoll"""
        try:
            result = await self._send_request("tools/list")
            
            if result:
                mcp_tools = result.get("tools", [])
                
                # Konvertiere MCP Tools zu Gemini Format
                self.tools = []

                # Hilfsfunktion zur Bereinigung inkompatibler JSON-Schema Felder
                def sanitize_schema(schema):
                    if isinstance(schema, dict):
                        allowed = {"type", "properties", "required", "description", "items"}
                        clean = {k: sanitize_schema(v) for k, v in schema.items() if k in allowed}
                        return clean
                    elif isinstance(schema, list):
                        return [sanitize_schema(i) for i in schema]
                    return schema
                
                for tool in mcp_tools:
                    gemini_tool = {
                        "name": tool.get("name"),
                        "description": tool.get("description", "")
                    }
                    
                    # Konvertiere inputSchema zu parameters
                    input_schema = tool.get("inputSchema", {})
                    if input_schema:
                        parameters = {
                            "type": input_schema.get("type", "object"),
                            "properties": sanitize_schema(input_schema.get("properties", {})),
                            "required": input_schema.get("required", [])
                        }
                        gemini_tool["parameters"] = parameters
                    
                    self.tools.append(gemini_tool)
                
                logger.info(f"‚úÖ {len(self.tools)} Tools geladen und konvertiert")
                logger.debug(f"Gemini Tools: {json.dumps(self.tools, indent=2)}")
                
                return self.tools
            else:
                logger.error("Keine Tools erhalten")
                return []
            
        except Exception as e:
            logger.error(f"Fetch Tools Error: {e}")
            return []
    
    async def call_tool(self, tool_name: str, parameters: dict):
        """Rufe Tool √ºber MCP Protokoll auf"""
        try:
            result = await self._send_request("tools/call", {
                "name": tool_name,
                "arguments": parameters
            })
            
            if result:
                # MCP gibt content array zur√ºck
                content = result.get("content", [])
                
                # Extrahiere Text aus content
                output_text = ""
                for item in content:
                    if isinstance(item, dict) and "text" in item:
                        output_text += item["text"]
                
                return {
                    "success": True,
                    "output": output_text or json.dumps(result)
                }
            else:
                return {
                    "success": False,
                    "error": "Keine Response vom Gateway"
                }
            
        except Exception as e:
            logger.exception(f"Tool Call Error")
            return {"success": False, "error": str(e)}
    
    async def disconnect(self):
        if self.client:
            await self.client.aclose()

#class DockerMCPClient:
#    """Kommuniziert mit laufendem Docker MCP Gateway"""
#    
#    def __init__(self):
#        self.tools = []
#    
#    async def connect(self):
#        """Pr√ºft ob Gateway l√§uft"""
#        try:
#            # Pr√ºfe ob Gateway l√§uft via CLI
#            result = subprocess.run(
#                ["docker", "mcp", "tools", "list"],
#                capture_output=True,
#                text=True,
#                timeout=5
#            )
#            
#            if result.returncode == 0:
#                logger.info("‚úÖ Docker MCP Gateway l√§uft")
#                return True
#            else:
#                logger.error(f"Gateway nicht verf√ºgbar: {result.stderr}")
#                return False
#                
#        except Exception as e:
#            logger.error(f"Gateway Check Error: {e}")
#            return False
#    
#    async def fetch_tools(self):
#        """Holt Tools vom Gateway via CLI"""
#        try:
#            result = subprocess.run(
#                ["docker", "mcp", "tools", "list", "--format", "json"],
#                capture_output=True,
#                text=True,
#                check=True
#            )
#            
#            tools_data = json.loads(result.stdout)
#            self.tools = []
#            
#            for tool in tools_data:
#                self.tools.append({
#                    "name": tool["name"],
#                    "description": tool.get("description", ""),#
#                    "parameters": tool.get("inputSchema", {
#                        "type": "object",
#                        "properties": {},
#                        "required": []
##                    })
#                })
#            
#            logger.info(f"‚úÖ {len(self.tools)} Tools vom Gateway geladen")
#            return self.tools
#            
#        except subprocess.CalledProcessError as e:
#            logger.error(f"Tools list failed: {e.stderr}")
#            return []
#        except Exception as e:
#            logger.error(f"Fetch Tools Error: {e}")
#            return []
#    
#    async def call_tool(self, tool_name: str, parameters: dict):
#        """F√ºhrt Tool via Gateway CLI aus"""
#        try:
#            # Baue Command - Parameter m√ºssen als JSON-String √ºbergeben werden
#            cmd = ["docker", "mcp", "tools", "call", tool_name]
#            
#            # WICHTIG: Parameter als stdin √ºbergeben, nicht als --input flag
#            result = subprocess.run(
#                cmd,
#                input=json.dumps(parameters),  # <-- Als stdin!
#                capture_output=True,
#                text=True,
#                timeout=30
#            )
#            
#            if result.returncode != 0:
#                logger.error(f"Tool {tool_name} failed: {result.stderr}")
#                return {"success": False, "error": result.stderr}
#            
#            return {
#                "success": True,
#                "output": result.stdout.strip()
#            }
#            
#        except subprocess.TimeoutExpired:
#            logger.error(f"Tool {tool_name} timeout")
#            return {"success": False, "error": "Tool call timeout"}
#        except Exception as e:
#            logger.error(f"Tool Call Error: {e}")
#            return {"success": False, "error": str(e)}
#    
#    async def disconnect(self):
#        """Cleanup (nichts zu tun)"""
#        pass



# --- Gemini Client ---
class GeminiClient:
    """Client f√ºr Google Gemini API mit Streaming-Unterst√ºtzung"""

    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"
        self.tools = []
    
    def register_tools(self, tools: list):
        """Registriere MCP Tools f√ºr Gemini"""
        self.tools = tools
        logger.info(f"Registriert {len(tools)} MCP Tools f√ºr Gemini")

    async def _stream_gemini_response(self, client, url, params, headers, body, contents_list, out_function_calls: list):
        """
        Interne Hilfsfunktion, um eine einzelne API-Anfrage zu streamen
        und die Antworten (Text und Tool-Calls) zu sammeln.
        
        MODIFIZIERT: Akzeptiert 'out_function_calls'-Liste und entfernt 'return'.
        """
        collected_text = ""
        # function_calls = []  <- ENTFERNT (wir verwenden out_function_calls)
        
        # Die Antwort der KI (ob Text oder Tool-Call) muss f√ºr den Verlauf gespeichert werden
        model_response_part = {"role": "model", "parts": []}

        async with client.stream("POST", url, json=body, headers=headers, params=params) as resp:
            if resp.status_code >= 400:
                error = await resp.aread()
                logger.error(f"Gemini Error {resp.status_code}: {error.decode()}")
                resp.raise_for_status()
            
            async for line in resp.aiter_lines():
                if not line or not line.startswith("data: "):
                    continue
                
                data_str = line[6:].strip()
                if data_str == "[DONE]":
                    break
                
                try:
                    event = json.loads(data_str)
                except json.JSONDecodeError:
                    logger.warning(f"Konnte JSON nicht dekodieren: {data_str}")
                    continue
                
                candidates = event.get("candidates", [])
                if not candidates:
                    continue
                
                content = candidates[0].get("content", {})
                parts = content.get("parts", [])
                
                for part in parts:
                    # Text sammeln UND streamen
                    if "text" in part:
                        text = part["text"]
                        collected_text += text
                        model_response_part["parts"].append({"text": text})
                        yield text  # Stream an User
                    
                    # Function Calls sammeln
                    if "functionCall" in part:
                        model_response_part["parts"].append(part)
                        
                        func = part["functionCall"]
                        # MODIFIZIERT: F√ºge zur 'out'-Liste hinzu
                        out_function_calls.append({
                            "name": func.get("name"),
                            "args": func.get("args", {})
                        })

        # F√ºge die gesamte Antwort der KI (Text und/oder Tool-Calls) zum Verlauf hinzu
        if model_response_part["parts"]:
            contents_list.append(model_response_part)

    async def generate_with_tools(self, contents: list, mcp_client, emotion: str) -> AsyncGenerator[str, None]:
        """
        Streamt Text + Tool Calls. Verwendet und aktualisiert den √ºbergebenen 'contents' in-place.
        Die aktuelle Emotion wird als TEMPOR√ÑRER Eintrag in 'contents' hinzugef√ºgt.
        """
        
        url = f"{self.base_url}/models/{self.model}:streamGenerateContent"
        params = {"key": self.api_key, "alt": "sse"}
        headers = {"Content-Type": "application/json"}
        tool_config = {"function_declarations": self.tools} if self.tools else None
        
        # HIER ERFOLGT DIE VORBEREITUNG DER INHALTE VOR DER ERSTEN RUNDE
        # -----------------------------------------------------------------------
        
        # 1. F√ºge die aktuelle User-Nachricht hinzu (wird vom Endpunkt gemacht)
        # 2. F√ºge die TEMPOR√ÑRE Emotions-Info hinzu, damit Gemini diese EINE Runde beachtet.
        # WICHTIG: Da diese Logik VOR dem Gemini-Aufruf erfolgt, muss der Endpunkt
        # nur den History-Teil des Contents √ºbergeben. Wir verschieben das Anf√ºgen
        # der User-Nachricht HIERHER.
        
        # Der User-Text ist der letzte Eintrag im contents-Array, den wir hier extrahieren.
        user_text_entry = contents.pop() 
        current_user_text = user_text_entry["parts"][0]["text"]

        # F√ºge die aktuelle, dynamische Emotion als tempor√§ren Kontext hinzu
        contents.append({"role": "user", "parts": [{"text": f"Aktuelle Stimmung: {emotion}"}]})
        contents.append({"role": "model", "parts": [{"text": "Ich werde darauf achten."}]})
        
        # F√ºge die eigentliche User-Nachricht hinzu
        contents.append(user_text_entry)
        
        # -----------------------------------------------------------------------
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            
            # --- Schritt 1: Runde 1 (Aktueller Verlauf + TEMPOR√ÑRER Emotion -> KI) ---
            logger.info(f"Starte Runde 1. Aktuelle dynamische Emotion: {emotion}")
            body_r1 = {
                "contents": contents, 
                "tools": [tool_config] if tool_config else []
            }

            # Rufe die Stream-Funktion auf
            # Sie `yield`et Text direkt an den User und gibt Tool-Calls zur√ºck
            function_calls = []

            async for text_chunk in self. _stream_gemini_response(
                client, url, params, headers, body_r1, contents, function_calls
            ):
                yield text_chunk # Streamt den Text-Chunk weiter an den User

            # --- Schritt 3: Pr√ºfen, ob Tools aufgerufen wurden ---
            if not function_calls:
                logger.info("Runde 1: Keine Tool-Calls. Antwort ist final.")
                return  # Fertig, aller Text wurde bereits gestreamt

            # --- Schritt 4: Tool-Ausf√ºhrung (Zwischenschritt) ---
            logger.info(f"Runde 1: F√ºhre {len(function_calls)} Tool Call(s) aus.")
            
            for call in function_calls:
                tool_name = call["name"]
                tool_args = call["args"]
                
                # Optional: Feedback an den User, dass etwas passiert
                yield f"\n\n[üîß Denke nach... (Rufe Tool {tool_name} auf)]\n"
                logger.info(f"Tool Call: {tool_name}({tool_args})")
                
                result = await mcp_client.call_tool(tool_name, tool_args)
                
                # *** DAS IST DER WICHTIGSTE TEIL ***
                # Formatiere das Tool-Ergebnis f√ºr die API (Runde 2)
                
                tool_result_data = {}
                if result.get("success"):
                    output = result.get("output", "")
                    # Versuche, das Ergebnis als JSON zu laden, da MCP oft JSON-Strings liefert
                    try:
                        tool_result_data = json.loads(output)
                    except (json.JSONDecodeError, TypeError):
                        # Wenn es kein JSON ist, verpacke es einfach
                        tool_result_data = {"content": str(output)}
                else:
                    error = result.get("error", "Unbekannter Fehler")
                    tool_result_data = {"error": error}
                    logger.error(f"Tool {tool_name} fehlgeschlagen: {error}")

                # F√ºge das Tool-Ergebnis zum Verlauf (contents) hinzu
                contents.append({
                    "role": "tool",
                    "parts": [
                        {
                            "functionResponse": {
                                "name": tool_name,
                                "response": tool_result_data
                            }
                        }
                    ]
                })

            # --- Schritt 5: Runde 2 (Tool-Ergebnisse -> KI) ---
            logger.info("Starte Runde 2: Sende Tool-Ergebnisse an Gemini f√ºr finale Antwort.")
            
            # Der Body enth√§lt jetzt den *gesamten* Verlauf
            body_r2 = {
                "contents": contents,
                "tools": [tool_config] if tool_config else [] # Tools m√ºssen erneut gesendet werden
            }

            # --- Schritt 6: Finale Antwort streamen ---
            # Wir rufen dieselbe Stream-Funktion erneut auf.
            # Diesmal wird der `yield`ete Text die finale, zusammengefasste Antwort sein.
            # Eventuelle (unerwartete) Tool-Calls in Runde 2 ignorieren wir hier.
            async for text_chunk in self. _stream_gemini_response(
                client, url, params, headers, body_r2, contents, [] 
            ):
                yield text_chunk
            
            logger.info("Runde 2: Finale Antwort gestreamt.")

# Globale Clients
coach = CppCoachClient(CPP_COACH_URL)
gemini = GeminiClient(GEMINI_API_KEY, GEMINI_MODEL)
mcp_client = MCPGatewayClient(gateway_host="localhost", gateway_port=PORT_MCP_GATEWAY)
#mcp_client = DockerMCPClient() 
memory_client = ConversationMemory(REDIS_URL)


@app.on_event("startup")
async def startup():
    # Verbinde Memory
    await memory_client.connect() 
    # Verbinde mit Gateway
    await mcp_client.connect()
    
    # Hole Tools
    tools = await mcp_client.fetch_tools()
    if tools:
        gemini.register_tools(tools)
        logger.info("üöÄ Gizmo Server ready mit Docker MCP Tools!")
    else:
        logger.warning("‚ö†Ô∏è Keine MCP Tools verf√ºgbar")

@app.on_event("shutdown")
async def shutdown():
    await mcp_client.disconnect()

# API Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health Check f√ºr HA Integration"""
    
    # Pr√ºfe C++ Coach
    coach_available = False
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get(f"{CPP_COACH_URL}/health")
            coach_available = response.status_code == 200
    except:
        pass
    
    return HealthResponse(
        status="ok",
        cpp_coach_available=coach_available,
        gemini_configured=GEMINI_API_KEY != "your-api-key-here"
    )

@app.get("/debug/gemini")
async def debug_gemini():
    """Debug Endpoint - Test Gemini direkt"""
    logger.info("Debug: Teste Gemini direkt")
    
    test_prompt = "Du bist ein freundlicher Assistent."
    test_text = "Sage hallo in einem kurzen Satz."
    
    full_response = ""
    chunk_count = 0
    
    try:
        async for chunk in gemini.generate_stream(test_prompt, test_text):
            full_response += chunk
            chunk_count += 1
            logger.info(f"Debug Chunk {chunk_count}: {chunk}")
        
        return {
            "success": True,
            "chunks": chunk_count,
            "response": full_response,
            "length": len(full_response)
        }
    except Exception as e:
        logger.exception("Debug Gemini failed")
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/api/conversation", response_model=ConversationResponse)
# --- Haupt-Funktion f√ºr non-streaming Konversation ---
async def process_conversation(request: ConversationRequest) -> ConversationResponse:
    """Non-streaming Hauptendpoint mit Server-Side Memory, dynamischer Emotion (nicht gespeichert)."""
    logger.info(f"Konversation {request.conversation_id}: {request.text}")

    try:
        # 1. Hol den Prompt und Emotion vom Coach (dynamisch)
        enriched = await coach.get_enriched_prompt()
        prompt = str(enriched["prompt"])
        emotion = str(enriched["emotion"])
        system_prompt = prompt 

        # 2. Lade den persistenten Verlauf aus Redis
        contents = await memory_client.load_history(request.conversation_id)
        
        # 3. Wenn der Verlauf leer ist, initialisiere nur den persistenten System-Prompt
        if not contents:
            logger.info(f"Starte Konversation {request.conversation_id}: Initialisiere System-Prompt.")
            
            # NUR DER PERSISTENTE SYSTEM-PROMPT WIRD GESPEICHERT
            contents.append({"role": "user", "parts": [{"text": system_prompt}]}) 
            contents.append({"role": "model", "parts": [{"text": "Verstanden, ich bin bereit."}]})
        
        # 4. F√ºge die aktuelle User-Nachricht hinzu (persistent)
        # Die eigentliche User-Anfrage wird ZUERST hinzugef√ºgt.
        contents.append({"role": "user", "parts": [{"text": request.text}]})

        # 5. F√ºhre Gemini-Generierung durch
        full_response = ""
        # Wir √ºbergeben den Verlauf, den MCP Client und die AKTUELLE Emotion
        async for chunk in gemini.generate_with_tools(contents, mcp_client, emotion): 
            full_response += chunk

        if not full_response:
            full_response = "Entschuldigung, ich konnte keine Antwort generieren."

        # 6. Bereinige tempor√§re Eintr√§ge im Verlauf
        clean_contents = []
        for entry in contents:
            # Ignoriere tempor√§re Emotionseintr√§ge
            if entry.get("role") == "user" and entry["parts"][0].get("text", "").startswith("Aktuelle Stimmung:"):
                continue
            # Ignoriere den KI-Eintrag, der nur "Ich werde darauf achten." sagt
            if entry.get("role") == "model" and entry["parts"][0].get("text", "") == "Ich werde darauf achten.":
                continue
            # Tool-Ausgaben sind nur tempor√§r f√ºr den Lauf
            if entry.get("role") == "tool":
                continue
            clean_contents.append(entry)

        # 7. Speichere den bereinigten Verlauf zur√ºck
        await memory_client.save_history(request.conversation_id, clean_contents)
        logger.info(f"üíæ Verlauf f√ºr {request.conversation_id} gespeichert: {len(clean_contents)} Eintr√§ge")

        logger.info(f"Antwort generiert. Neuer bereinigter Verlauf: {len(contents)} Teile.")
        return ConversationResponse(response=full_response, conversation_id=request.conversation_id)

    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Fehler bei Konversationsverarbeitung")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/conversation/stream")
# --- Streaming-Haupt-Funktion ---
async def process_conversation_stream(request: ConversationRequest):
    """Streaming Endpoint: sendet Text-Events St√ºck f√ºr St√ºck an Client"""
    async def event_generator():
        try:
            enriched = await coach.get_enriched_prompt(request.text)
            system_prompt = enriched["prompt"] + enriched["emotion"]

            print(system_prompt)

            async for chunk in gemini.generate_stream(system_prompt, request.text):
                yield f"data: {json.dumps({'text': chunk})}\n\n"

            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.exception("Stream Error")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


if __name__ == "__main__":
    import uvicorn
    
    logger.info("üöÄ Starte Gizmo Python LLM Server")
    logger.info(f"   C++ Coach: {CPP_COACH_URL}")
    logger.info(f"   Gemini Modell: {GEMINI_MODEL}")
    logger.info(f"   Gemini API KEY: {GEMINI_API_KEY}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=PORT_PYTHON_SERVER,
        log_level="info"
    )